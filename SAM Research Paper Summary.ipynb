{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9333c355-5f40-43ed-a6e3-a77b0f92041b",
   "metadata": {},
   "source": [
    "# Segment Anything Model (SAM) Research Paper Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b8bf8-952c-48de-a0d3-c9685d3e9d90",
   "metadata": {},
   "source": [
    "This notebook is my personal research notebook for understanding the ins and outs of SAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a730a96-400f-4177-828c-ccb736f2d12b",
   "metadata": {},
   "source": [
    "Original paper: https://arxiv.org/pdf/2304.02643v1.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784baf4d-d949-4810-b1ee-775435267d59",
   "metadata": {},
   "source": [
    "<img src=\"images/paper_hero.jpg\" >\n",
    "\n",
    "Figure 1: We aim to build a foundation model for segmentation by introducing three interconnected components: \n",
    "\n",
    "1. a promptable segmentation task\n",
    "2. a segmentation model (SAM) that powers data annotation and enables zero-shot transfer to a range of tasks via prompt engineering\n",
    "3. a data engine for collecting SA-1B, our dataset of over 1 billion masks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ba91e2-f81b-46ec-baa9-4d6316f967ab",
   "metadata": {},
   "source": [
    "# Abstract Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d9b8ce-67dd-4b42-84a1-4df51a965ae8",
   "metadata": {},
   "source": [
    "- They created the largest segmentation datasets to date with over 1 billion masks on 11M images.\n",
    "- Model is designed and trained to be promptable so it can transfer zero-shot to new image distributions and tasks.\n",
    "- Zero-shot performance is competitive with fully supervised results.\n",
    "- They have released both the model and the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d5bba1-6b76-4866-899f-970f199cd2c9",
   "metadata": {},
   "source": [
    "# Introduction Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbc4806-7473-4e21-aeae-54de8befe133",
   "metadata": {},
   "source": [
    "- Foundation models can generalize to tasks and data distributions beyond those seen during training.\n",
    "- Empirical trends show this behavior improving with model scale, dataset size and total training compute.\n",
    "- This paper's goal is to build a foundation model for image segmentation (generalized foundational model).\n",
    "\n",
    "### 1. Task: \n",
    "\n",
    "They defined a promptable segmentation task that is general enough to provide a powerful pretraining objective and to enable a wide range of downstream applications.\n",
    "\n",
    "Inspired by NLP foundation models for zero-shot and few-shot learning, they propose *promptable segmentation task*, where the goal is to return a valid segmentation mask given any segmentation prompt.\n",
    "\n",
    "A prompt specifies what to segment in an image and can contain spatial or text information identifying an object.\n",
    "\n",
    "The requirement of a valid output mask means that even when a prompt is ambiguous and could refer to multiple objects (for example, a point on a shirt may indicate either the shirt or the person wearing it), the output should be a reasonable mask for at least one of those objects. We use the promptable segmentation task as both a pre-training objective and to solve general downstream segmentation tasks via prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179dd199-0eb8-466e-88fc-48ea8c47f46a",
   "metadata": {},
   "source": [
    "### 2. Model:\n",
    "\n",
    "The task requires a model that supports flexible prompting and can output segmentation masks in real-time when prompted to allow for interactive use.\n",
    "\n",
    "The model must support flexible prompts, needs to compute masks in amortized real-time to allow interactive use, and must be ambiguity-aware. \n",
    "\n",
    "A single design satisfies all three constraints: \n",
    "\n",
    "- a powerful image encoder computes an image embedding\n",
    "- a prompt encoder embeds prompts\n",
    "- a lightweight mask decoder predicts segmentation masks. This is the Segment Anything Model.\n",
    "\n",
    "By separating SAM into an image encoder and a fast prompt encoder / mask decoder, the same image embedding can be reused (and its cost amortized) with different prompts.\n",
    "\n",
    "**Algorithm of model:** \n",
    "\n",
    "Given an image embedding, the prompt encoder and mask decoder predict a mask from a prompt in ∼50ms in a web browser. We focus on point, box, and mask prompts, and also present initial results with free-form text prompts. To make SAM ambiguity-aware, we design it to predict multiple masks for a single prompt allowing SAM to naturally handle ambiguity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea067df-2225-4caa-bbee-9778b0be8951",
   "metadata": {},
   "source": [
    "### 3. Data Engine:\n",
    "\n",
    "Need diverse large scale sources of data. Therefore, they built a data engine, i.e., they iterated between using their efficient model to assist in data collection and used the newly collected data to improve the model.\n",
    "\n",
    "Their data engine has three stages: \n",
    "\n",
    "- assisted-manual - SAM assists annotators in annotating masks (classic interactive segmentation setup)\n",
    "- semi-automatic - SAM automatically generates masks for a subset of objects by prompting it with likely object locations and annotators focus on annotating the remaining objects.\n",
    "- fully automatic - They prompted SAM with a regular grid of foreground points, yielding on average ~100 high quality masks per image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8273902-6dd1-445a-a605-29060bee7d90",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "\n",
    "- Using a diverse new suite of 23 segmentation datasets, they found SAM produces high-quality masks from a single foreground point, often only slightly below that of the manually annotated ground truth.\n",
    "- They found consistently strong quantitative and qualitative results on a variety of downstream tasks under a zero-shot transfer protocol using prompt engineering, including edge detection, object proposal generation, instance segmentation, and a preliminary exploration of text-to-mask prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a667b4f-a18b-4341-aa93-ceff5dbc7943",
   "metadata": {},
   "source": [
    "# Segment Anything Task\n",
    "\n",
    "Inspiration: **Next token prediction task** is used for **foundation model pre-training** and to solve downstream tasks via prompt engineering. \n",
    "\n",
    " - A prompt can be foreground/background points, a rough box or mask, free-form text, or in general, what to segment in an image. \n",
    "\n",
    "Then, the **promptable segmentation task** is to return a **valid** segmentation mask given any prompt.\n",
    "\n",
    "Valid = Even when a prompt is ambigous and could refer to multiple objects, the output should be a reasonable mask for at least one of those objects.\n",
    "\n",
    "- The promptable segmentation task suggests a natural pre-training algorithm that simulates a sequence of prompts (e.g., points, boxes, masks) for each training sample and compares the model’s mask predictions against the ground truth.\n",
    "\n",
    "Unlike interactive segmentation whose aim is to eventually predict a valid mask after enough user input, their aim is to always predict a valid mask even when the prompt is ambiguous. This requires specialized modeling and training loss choices.\n",
    "\n",
    "- Their pre-training task endows the model with the ability to respond appropriately to any prompt at inference time, and thus downstream tasks can be solved by engineering appropriate prompts.\n",
    "\n",
    "For example, if one has a bounding box detector for cats, cat instance segmentation can be solved by providing the detector’s box output as a prompt to their model. In general, a wide array of practical segmentation tasks can be cast as prompting.\n",
    "\n",
    "- Related tasks: Interactive segmentation, edge detection, super pixelization, object proposal generation, foreground segmentation, semantic segmentation, instance segmentation, panoptic segmentation, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b03fe96-f253-489e-b7d3-f8372c402023",
   "metadata": {},
   "source": [
    "# Segment Anything Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cbde76-bbdf-4577-a7a9-874a633f3970",
   "metadata": {},
   "source": [
    "<img src = \"images/segment_anything_model.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6d877e-34a7-422a-baa6-7a38efff523c",
   "metadata": {},
   "source": [
    "**Image encoder** - MAE pre-trained Vision Transformer (ViT) minimally adapted to process higher resolution inputes. The encoder runs once per image and can be applied prior to prompting the model.\n",
    "\n",
    "**Prompt encoder** - Considers sparse (points, boxes, text) and dense (masks) prompts.\n",
    "\n",
    "Points and boxes are represented by positional encodings summed with learned embeddings for each prompt type and free-form text with an off-the-shelf text encoder from CLIP.\n",
    "\n",
    "Dense (masks) prompts are embedded using convolutions and summed element-wise with the image embedding\n",
    "\n",
    "**Mask decoder** - Maps image embedding, prompt embedding, and an output token to a mask (Modification of a Transformer decoder block followed by a dynamic mask prediction head).\n",
    "\n",
    "The modified decoder block uses prompt self-attention and cross-attention in two directions (prompt-to-image embedding and vice-versa) to update all embedding.\n",
    "\n",
    "After running two blocks, they upsample the image embedding and an MLP maps the output token to a dynamic linear classifier, which then computes the mask foreground probability at each image location.\n",
    "\n",
    "**Resolving ambiguity** - Outputs 3 masks to average multiple valid masks (whole, part and subpart). They backprop only the minimum loss over masks. To rank masks, the model predicts a confidence score (estimated IoU) for each mask.\n",
    "\n",
    "**Efficiency** - Given a precomputed image embedding, the prompt encoder and mask decoder run in a web browser, on CPU, in ∼50ms.\n",
    "\n",
    "**Loss** - Linear combination of focal and dice loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
