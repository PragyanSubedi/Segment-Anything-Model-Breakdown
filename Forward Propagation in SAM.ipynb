{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f30ac17f-4975-47be-ba9a-cbb9622d5bc2",
   "metadata": {},
   "source": [
    "This notebook is an explanation of the `notebooks/automatic_mask_generator_example.ipynb` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7379616-ec37-40d5-b3b8-c48407d61873",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **Algorithm Overview**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc53f87a-298f-4981-8d50-22d98678d1cf",
   "metadata": {},
   "source": [
    "1. Initialize `_build_sam` using `build_sam_vit_h` wrapper.\n",
    "\n",
    "    def build_sam_vit_h(checkpoint=None):\n",
    "        return _build_sam(\n",
    "            encoder_embed_dim=1280,\n",
    "            encoder_depth=32,\n",
    "            encoder_num_heads=16,\n",
    "            encoder_global_attn_indexes=[7, 15, 23, 31],\n",
    "            checkpoint=checkpoint,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ff698-5461-4b4b-afc2-257d5540f855",
   "metadata": {},
   "source": [
    "2. The `build_sam` function initializes a `SAM` object. \n",
    "\n",
    "The SAM object takes in\n",
    "\n",
    "- an `image_encoder`, which is a Vision Transformer, \n",
    "- a `prompt_encoder`, which encodes the given prompt\n",
    "- a `mask_decoder`, which Maps image embedding, prompt embedding, and an output token to a mask (Modification of a Transformer decoder block followed by a dynamic mask prediction head).\n",
    "\n",
    "3. Pass the `SAM` object to the `SamAutomaticMaskGenerator` and call the `generate` function for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165fdd4-e684-4768-bfa5-7f8ad1d3fc27",
   "metadata": {},
   "source": [
    "# **Forward Propagation in SAM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6bb4e3-996f-48a6-b76d-7ddaa8710a7f",
   "metadata": {},
   "source": [
    "<img src = \"https://github.com/PragyanSubedi/Segment-Anything-Model-Breakdown/blob/main/images/segment_anything_model.PNG?raw=true\" >\n",
    "\n",
    "The code for this can be found in `segment_anything.modeling.sam`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69865fc9-46c1-44a5-9aba-44f3e996390f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 1: Image pre-processing\n",
    "\n",
    "```input_images = torch.stack([self.preprocess(x[\"image\"]) for x in batched_input], dim=0)```\n",
    "\n",
    "1. The pre-process normalizes the color (Standardization) by subtracting each pixel of an image by the batch's mean and dividing the resultant by the batch's standard deviation. \n",
    "2. The image is also padded in order to set as the input image size of the image encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e93034-38b2-47e5-b43e-40d21233b054",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Step 2: Create image embeddings using Vision Transformer (ViT) Image Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5897096c-cbb0-4579-86d9-b485bd2089c1",
   "metadata": {},
   "source": [
    "```image_embeddings = self.image_encoder(input_images)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baedc21f-8345-4019-b0a5-da750bc5cf7b",
   "metadata": {},
   "source": [
    "Original paper of ViT: https://arxiv.org/pdf/2010.11929v2.pdf\n",
    "\n",
    "**Key point:**\n",
    "\n",
    "```\n",
    "Inspired by the Transformer scaling successes in NLP, we experiment with applying a standard Transformer directly to images, with the fewest possible modifications. To do so, we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer. Image patches are treated the same way as tokens (words) in an NLP application. We train the model on image classification in supervised fashion.\n",
    "```\n",
    "\n",
    "<img src =\"https://github.com/PragyanSubedi/Segment-Anything-Model-Breakdown/blob/main/images/vit_architecture.PNG?raw=true\" width=80%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7bc22f-f1f5-4b6e-8a01-11a33435b39b",
   "metadata": {},
   "source": [
    "**For each image, x:**\n",
    "\n",
    "1. Create a patch embedding of image x with pre-defined patch size of 16 x 16. The stride is set as 16 x 16 as well. The number of input image channels are 3 since we are working with RGB images and the embedding dimension is 1280 (vit_h).\n",
    "2. Add a positional embedding to x if positional embedding is not None.\n",
    "3. Create 32 blocks of x. \n",
    "\n",
    "\n",
    "```\n",
    "def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        # Window partition\n",
    "        if self.window_size > 0:\n",
    "            H, W = x.shape[1], x.shape[2]\n",
    "            x, pad_hw = window_partition(x, self.window_size)\n",
    "\n",
    "        x = self.attn(x)\n",
    "        # Reverse window partition\n",
    "        if self.window_size > 0:\n",
    "            x = window_unpartition(x, self.window_size, pad_hw, (H, W))\n",
    "\n",
    "        x = shortcut + x\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "```\n",
    "\n",
    "In each block, normalize x. If windows_size is > 0, create a window partition else do not. Pass through a multiheaded attention block. Reverse window partition. Add it with the original value of x as it came in the block. Pass it through the MLP layer after normalization and then add it with the value of x before normalization.\n",
    "\n",
    "This gives the `image_embeddings`.\n",
    "\n",
    "\n",
    "4. Pass it through the neck: conv2d->layernorm->conv2d->layernorm. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8410c588-1f24-4c0e-a894-2eccef020326",
   "metadata": {},
   "source": [
    "#### **Step 3: Use a prompt encoder to get `sparse_embeddings` and `dense_embeddings`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8a3b6-3c9e-4c68-8281-11b1a6f1fd39",
   "metadata": {},
   "source": [
    "```\n",
    "for image_record, curr_embedding in zip(batched_input, image_embeddings):\n",
    "            if \"point_coords\" in image_record:\n",
    "                points = (image_record[\"point_coords\"], image_record[\"point_labels\"])\n",
    "            else:\n",
    "                points = None\n",
    "            sparse_embeddings, dense_embeddings = self.prompt_encoder(\n",
    "                points=points,\n",
    "                boxes=image_record.get(\"boxes\", None),\n",
    "                masks=image_record.get(\"mask_inputs\", None),\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40933726-7f5e-4793-b6dc-4be31e9b8567",
   "metadata": {},
   "source": [
    "Loop across each single image record and its image embeddings.\n",
    "\n",
    "- If the image record contains point coordinates `point_coords`, set points as a tuple of `point_coords` and `point_labels`. Else, set `points` as None.\n",
    "\n",
    "- Then, create a sparse (for points and boxes) and dense embeddings (for masks) using the `prompt_encoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368cb442-bcb4-4868-a09d-3942c120fbe3",
   "metadata": {},
   "source": [
    "#### **Step 4: Generate `low_res_masks` and `iou_predictions`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b319b-5541-41a5-a066-50faec4cca79",
   "metadata": {},
   "source": [
    "```\n",
    "low_res_masks, iou_predictions = self.mask_decoder(\n",
    "                image_embeddings=curr_embedding.unsqueeze(0),\n",
    "                image_pe=self.prompt_encoder.get_dense_pe(),\n",
    "                sparse_prompt_embeddings=sparse_embeddings,\n",
    "                dense_prompt_embeddings=dense_embeddings,\n",
    "                multimask_output=multimask_output,\n",
    "            )\n",
    "```\n",
    "\n",
    "Use the mask decoder that creates predictions for masks and Intersection over Union (IoU).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7260dd19-9fa0-4ebb-a539-bbc6897036cf",
   "metadata": {},
   "source": [
    "#### **Step 5: Postprocess masks and provide output predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a5eb94-77e6-43a2-baa8-ca91ef8fff9d",
   "metadata": {},
   "source": [
    "```\n",
    "masks = self.postprocess_masks(\n",
    "                low_res_masks,\n",
    "                input_size=image_record[\"image\"].shape[-2:],\n",
    "                original_size=image_record[\"original_size\"],\n",
    "            )\n",
    "masks = masks > self.mask_threshold\n",
    "outputs.append(\n",
    "    {\n",
    "        \"masks\": masks,\n",
    "        \"iou_predictions\": iou_predictions,\n",
    "        \"low_res_logits\": low_res_masks,\n",
    "    }\n",
    ")         \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa439eb-bac6-4c05-8d57-f6ba5d816fae",
   "metadata": {},
   "source": [
    "- Remove padding and upscale masks to original image size.\n",
    "- Check if mask is greater than mask threshold.\n",
    "- Create outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74fcabd-affe-4745-8ee7-7083ae7d4d78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
